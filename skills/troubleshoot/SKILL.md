---
name: troubleshoot
description: |
  Diagnose and fix issues in code, builds, deployments, and system behavior.
  Always traces root cause, forms hypotheses before fixing, and validates after.
  Operates with introspection markers and hypothesis-driven debugging.

  Use when:
  - User reports a bug, error, or unexpected behavior
  - Build or compilation is failing
  - Tests are failing or flaky
  - Performance has degraded
  - Deployment or environment issues
  - User says "fix", "debug", "broken", "failing", "not working", "error"

  Keywords: troubleshoot, debug, fix, error, bug, failing, broken, crash, exception, not working
argument-hint: "[issue/error description] [--frontend-verify]"
---

# Troubleshoot â€” Hypothesis-Driven Debugging & Fix

Systematic issue diagnosis with root cause tracing, validated fixes, and prevention guidance.

<essential_principles>

## Serena Think Checkpoints (Mandatory)

These three tools MUST be called at the specified points. Never skip them.

| Checkpoint | Tool | When | Purpose |
|------------|------|------|---------|
| Information Gate | `mcp__serena__think_about_collected_information` | After Phase 1 (Reproduce) and Phase 2 (Hypothesize) | Verify sufficient evidence before proceeding |
| Adherence Gate | `mcp__serena__think_about_task_adherence` | Before each code edit in Phase 4 (Fix) | Confirm fix aligns with identified root cause |
| Completion Gate | `mcp__serena__think_about_whether_you_are_done` | Before exiting Phase 5 (Verify) | Confirm fix is verified with evidence |

## Always Active

- **Hypothesis before fix**: Never guess-fix. Always form a hypothesis, gather evidence,
  then apply the fix. "ðŸ¤” I think X because Y" â†’ verify â†’ fix
- **Introspection markers**: Make debugging reasoning visible throughout:
  - ðŸ¤” Reasoning â€” "ðŸ¤” The stack trace points to a null ref in..."
  - ðŸŽ¯ Decision â€” "ðŸŽ¯ Root cause identified: missing null check at..."
  - âš¡ Performance â€” "âš¡ This N+1 query causes the slowdown"
  - ðŸ“Š Quality â€” "ðŸ“Š This fix also addresses the underlying design issue"
  - ðŸ’¡ Insight â€” "ðŸ’¡ This pattern is error-prone; consider refactoring"
- **Validate every fix**: Run lint/typecheck/test after each change. No unverified fixes
- **Destructive changes require confirmation**: Deleting files, resetting state, dropping data
- **No project-specific rules**: This skill works across all projects and AI agents

</essential_principles>

## Phase 1: Reproduce

Understand and reproduce the issue before diagnosing.

1. Parse the error description from user input
2. ðŸ¤” Identify the error type: bug / build / test / performance / deployment
3. Collect evidence:
   - Read error messages, stack traces, logs
   - Run the failing command to see the actual output
   - Check `git diff` or `git log` for recent changes that may have caused it
4. Confirm reproduction: "I can reproduce this by running X â†’ error Y"

**If cannot reproduce**: Ask user for more context before proceeding.

**Tools**: Bash, Read, Grep, Glob

**ðŸ”¶ `think_about_collected_information`** â€” Is the reproduction evidence sufficient?

## Phase 2: Hypothesize

Form hypotheses about the root cause â€” do not jump to fixing.

1. ðŸ¤” List 2-3 candidate hypotheses based on evidence:
   ```
   ðŸ¤” Hypothesis A: Missing dependency after package update
   ðŸ¤” Hypothesis B: Type mismatch from recent refactor
   ðŸ¤” Hypothesis C: Environment variable not set
   ```
2. ðŸŽ¯ Rank by likelihood based on evidence strength
3. Start investigating the most likely hypothesis first

**ðŸ”¶ `think_about_collected_information`** â€” Are hypotheses grounded in evidence?

## Phase 3: Investigate

Systematically verify or eliminate each hypothesis.

1. Read the relevant source code (trace from error location outward)
2. Follow the call chain: caller â†’ function â†’ dependencies
3. Check external library behavior with Context7 if the issue involves a framework/library
4. Narrow down to the root cause with evidence:
   ```
   ðŸŽ¯ Root cause: X confirmed. Evidence: [specific line/behavior]
   ```

**Tools**: Read, Grep, Glob, `mcp__serena__find_symbol`, `mcp__context7__query-docs`

## Phase 4: Fix

Apply the fix with adherence checks before each edit.

For each code change:

1. **ðŸ”¶ `think_about_task_adherence`** â€” Is this edit aligned with the identified root cause?
2. ðŸ“Š Describe the fix approach before editing:
   ```
   ðŸ“Š Fix: Change X to Y in file:line because [reason]
   ```
3. Apply the minimal fix (don't refactor unrelated code)
4. If fix requires destructive changes â†’ confirm with user first

**Tools**: Edit, Write, Bash

## Phase 5: Verify

Prove the fix works with concrete evidence. No fix is complete without verification.

### Standard Verification (always)

1. **Re-run the reproduction**: Execute the same command/action from Phase 1
   - Confirm the error no longer occurs
   - Record the output as evidence
2. **Quality checks** â€” run in parallel where possible:
   ```bash
   pnpm lint & pnpm typecheck & pnpm test & wait
   ```
3. **Evidence collection** â€” at least one of:
   - Console/log output showing the fix works
   - Test results (new or existing tests passing)
   - Screenshot of corrected behavior
   - User confirmation request ("Can you verify X now works?")

If any check fails â†’ return to Phase 3 with new evidence.

### `--frontend-verify` (when flag is provided)

Visual verification across platforms. Auto-detect platform from `package.json`:

| Dependency | Platform | Preflight | Verification Tool |
|------------|----------|-----------|-------------------|
| _(default)_ | Web | `kill-port <port> && pnpm dev` | Claude Chrome MCP (`tabs_context_mcp`, `read_page`, `take_screenshot`) |
| `electron` | Electron | `pnpm electron:dev` | Electron MCP (`mcp__electron__take_screenshot`) |
| `expo` / `react-native` | Mobile | `mcp__ios-simulator__open_simulator` | iOS Simulator MCP (`screenshot`, `ui_tap`, `ui_swipe`) |
| `commander` / `inquirer` / `oclif` | CLI | shell session | Shellwright MCP (TUI/CLI operation and output verification) |

**Frontend Verify Workflow:**

1. **Preflight**: Start dev server / app, confirm MCP connection
2. **Before screenshot**: Take screenshot of the broken state (if reproducible in UI)
3. **After fix screenshot**: Take screenshot of the corrected state
4. **Compare**: Present before/after to user for confirmation
5. **Judge**: All pass â†’ continue. Any fail â†’ return to Phase 4

### Completion Gate

**ðŸ”¶ `think_about_whether_you_are_done`** â€” Is the fix verified with evidence?

## Phase 6: Report

Summarize findings for the user.

1. **Root Cause**: What was wrong and why
2. **Fix Applied**: What was changed (files, lines)
3. **Evidence**: Verification results (logs, screenshots, test output)
4. **Prevention**: ðŸ’¡ How to avoid this in the future (optional, only if insightful)

## Examples

```
/troubleshoot "TypeError: Cannot read properties of undefined"
/troubleshoot build is failing after upgrading React
/troubleshoot tests pass locally but fail in CI --frontend-verify
/troubleshoot API response time doubled since last deploy
```

## Phase Flow Diagram

```
[Reproduce] â†’ ðŸ”¶ info gate
     â†“
[Hypothesize] â†’ ðŸ”¶ info gate
     â†“
[Investigate]
     â†“
   [Fix] â†’ ðŸ”¶ adherence gate (per edit)
     â†“
 [Verify] â†’ ðŸ”¶ completion gate
     â†“
 [Report]
```

## Boundaries

**Will:**
- Systematically trace root causes with evidence-based reasoning
- Apply minimal, validated fixes with hypothesis-driven debugging
- Verify fixes with concrete evidence (logs, screenshots, test results)
- Explain the debugging process transparently with introspection markers

**Will Not:**
- Apply fixes without understanding the root cause
- Make speculative changes hoping something works
- Mark a fix as complete without verification evidence
- Modify production systems without explicit user confirmation
